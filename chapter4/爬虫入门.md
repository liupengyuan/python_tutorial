1. 什么是爬虫

简而言之，爬虫就是一段能够获取互联网信息（数据）的程序/工具。

一般需要通过抓取网页来获取互联网的信息与数据。

网页本身就是一个本文文件，只不过这个文本文件是由特定规则和符号标记的(HTML,超文本标记语言)，称为超文本文件，也可称为网页源代码。这段文本经过浏览器的解析（各类图片视频等在此过程中从网页外部加载），就成为我们日常浏览的网页展示给我们的样子了。

因此，爬虫首先就是要获得网页这个文本文件，并进行解析。如果所需数据就直接在网页文本文件中，则可直接得到；如果所需数据在网页外部，则通过解析的结果得到数据所在地址，将该数据下载。

2. 最简爬虫
 
这里我们以优秀的python第三方库requests为例，该库是“唯一一个非转基因的Python HTTP库，人类可以安全享用”。其中HTTP(HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议，超文本文件传输时，必须遵守这个协议。非转基因是作者幽默的说法，表明该库是原汁原味符合python设计理念，易用易于理解。

输入以下代码，并观察执行结果。

```python
# coding: utf-8
# 示例代码 D-1

import requests

r = requests.get('https://github.com/liupengyuan/python_tutorial/blob/master/chapter1/0.md')
with open('0.md', 'w') as f:
    f.write(r.text)
```

示例代码D-1中：
- `import requests`，首先导入`requests`库
- `r = requests.get('https://github.com/liupengyuan/python_tutorial/blob/master/chapter1/0.md')`，调用requests的get方法，向网址https://github.com/liupengyuan/python_tutorial/blob/master/chapter1/0.md 发送获取(get)请求，该方法返回一个应答(Response)对象r，其中包含该网页的所有信息。
- `print(r.text)`，打印对象r的text属性，即网页的HTML文本。

打开`https://github.com/liupengyuan/python_tutorial/blob/master/chapter1/0.md`网页，在网页正文区域点击右键，选择查看源代码，会发现代码示例D-1确实已经获取/抓取这个网页的文本文件。至此，一个最简爬虫已经完成。

3. 静态定向爬虫基础

本小节中的爬虫是开始就指定了要抓取网页的地址（定向），要抓取的内容直接存在在网页源代码中（静态），因此称为静态定向爬虫。

- 任务1：抓取网易首页(www.163.com) 上的全部新闻，并以将所有新闻标题及新闻内容存入到一个文本文件中。

首先我们打开网易首页，在页面内copy一个新闻标题到剪贴板，然后用鼠标右键点击页面，并选择`查看源代码`(如用chrome浏览器可选择`检查`，进入开发者模式)。，会看到一片很乱的代码(其实就是html标记的文本)，如果有一些html基础(推荐教程：http://www.w3school.com.cn/html/index.asp)， 看起来就不会那么乱了。

可以对示例代码D-1稍加修改，就能够抓取到这个页面(代码)，但我们需要对这个网页源代码进行解析，定位到我们希望的内容，获取我们希望得到的数据。这里将使用优秀的第三方网页解析包Beautiful Soap (ver4)，该包已经随Anaconda安装，名称为bs4，并辅以对抓取页面的仔细分析。

在刚打开的源代码页按`Ctrl+F`查找刚才copy在剪贴板上的新闻标题，找到这个新闻标题的位置。这行大致是这样：` <li class="cm_fb"><a href="http://news.163.com/xxxxxx.html">yyyyyyyyy</a></li>`， 其中`xxxxx`及`yyyyyyy`随新闻标题不同而不同。其中，`yyyyyyyy`是新闻标题，`http://news.163.com/xxxxxx.html`是新闻标题对应的网页链接，访问该网页链接，就能进入该新闻的内容页面，因此这两项是我们感兴趣并希望抓取的内容。

我们在源代码页继续查看各个新闻标题，可以发现各个新闻其实都在标签对`<li>`及`</li>`之间。

输入以下代码，并观察执行结果。

```python
# coding: utf-8
# 示例代码 D-2

import requests
from bs4 import BeautifulSoup

r = requests.get('http://www.163.com')
f = open('news_163.txt', 'w', encoding = 'utf-8')

soap = BeautifulSoup(r.text, 'html.parser')
for model in soup.find_all('li'):
    f.write(str(model.contents))
    
f.close()
```
示例代码D-2中：
- `from bs4 import BeautifulSoup`，从bs4包中导入BeautifulSoup模块
- `soap = BeautifulSoup(r.text, 'html.parser')`，对网页文本，利用python内置的`html.parser`解析器，将`r.text`初始化为`BeautifulSoup`对象`soap`。
- `soup.find_all('li')`，调用`BeautifulSoup`对象`soap`的`find_all()`方法，找到标签`li`之间的所有内容，并返回一个结果集合(`ResultSet`)，集合中每一个元素为一个`Tag`对象，与html中对tag定义相同，此处即利用`<li>`及`</li>`进行标记的所有对象。
- 对每一个Tag对象model，取出其内容(contents属性,会返回一个list，内容是两个tag之间的所有内容)，并转换为字符串写入文件。

打开news_163.txt文件，观察内容，并找到与网页源文件对应的新闻标题与链接处，进行比较可以发现，示例代码D-2确实将所有新闻标题及相应链接得到了，但同时也获取了很多不需要的数据。

由于我们是找到了`<li>`及`</li>`进行标记的所有对象，而经过观察可以发现，这样的对象非常多，并非只有新闻标题与链接符合这一模式。我们只能更细致地分析网页源代码。

我们重新在网页源文件中查找新闻标题，进行观察，可以发现，新闻标题及链接均在`<ul class="cm_ul_round">`及`</ul>`之间(指向一种名称为`cm_ul_round`的样式)，且源文件中，其他非新闻标题的内容，均不在这种规定了`class`的`<ul>`标签之间，OK，找到关键标签了。
输入以下代码，并观察执行结果（查看所保存文件内容）。

```python
# coding: utf-8
# 示例代码 D-3

import requests
from bs4 import BeautifulSoup

r = requests.get('http://www.163.com')
f = open('news_163.txt', 'w', encoding = 'utf-8')

soup = BeautifulSoup(r.text, 'html.parser')

for model in soup.find_all(attrs={"class":"cm_ul_round"}):
    model = model.find_all('a')
    for text in model:
        f.write('{}:{}\n'.format(text.get('href'),text.string))
        
f.close()
```
示例代码 D-3中：
- ` soup.find_all(attrs={"class":"cm_ul_round"})`，find_all()中有个attrs 参数，可以定义一个字典参数来搜索包含特殊属性的tag，这个特殊属性`class="cm_ul_round"`以词典`{"class":"cm_ul_round"}`的形式传给attrs，就可以得到所有在`<ul class="cm_ul_round">`及`</ul>`之间的对象集合了。
- `model.find_all('a')`，得到其包含在标签`<a>`之间的对象集合。
- `text.get('href')`，利用tag对象的get()方法得到text对象的链接。
- `text.string`，利用tag对象的string属性，得到text对象的文本。

我们得到了首页上每个新闻标题及链接URL，我们下一步就是要利用这些URL来抓取新闻页面，并从中提取出正文。整个过程与前面示例代码D-3处理的过程基本类似，关键在于，找到正文在新闻网页源代码中所处的标签。

输入以下代码，并观察执行结果（查看所保存文件内容）。
```python
# coding: utf-8
# 示例代码 D-4

import requests
from bs4 import BeautifulSoup

# 第一部分，得到首页面新闻URL及标题，并存入dict
r = requests.get('http://www.163.com')
soup = BeautifulSoup(r.text, 'html.parser')
url_titles = {}

for model in soup.find_all(attrs={"class":"cm_ul_round"}):
    model = model.find_all('a')
    for text in model:
        try:
            url_titles[text.get('href')]=text.string
        except KeyError:
            pass

# 第二部分，对URL及标题的dict遍历，抓取每个新闻页面，提取正文并保存
f = open('news_163.txt', 'w', encoding = 'utf-8')

for url, title in url_titles.items():
    try:
        r = requests.get(url)
    except:
        pass
    soup = BeautifulSoup(r.text, 'html.parser')
    
    f.write('title:{}\n'.format(title))
    for model in soup.find_all(attrs={"class":"post_text"}):
        model = model.find_all('p')
        for text in model:
            if text.string:
                f.write(text.string)
    f.write('\n')
    
f.close()
```

示例代码D-4中：
- `url_titles[text.get('href')]=text.string`，这里以URL为键，以新闻标题为值，存入字典
- `soup.find_all(attrs={"class":"post_text"})`，通过观察源代码（或利用谷歌浏览器的检查），发现正文在`"class="cm_ul_round"`属性的标签之间，因此提取其间所有内容
- `model.find_all('p')`，提取`model`中所有标签`<p>与</p>`之间的内容

经过以上几个步骤，我们成功地将`www.163.com`首页所有新闻标题及正文抓取并存储到文件中了。当然，由于我们并没有特别仔细分析页面，最终抓取的文件中，仍然含有一部分不太正确的地方，留给读者自行解决。

- 任务2：抓取有道词典查询词的双语例句

通过在有道词典(dict.youdao.com)进行单词查询，发现对任意单词`xxxxx`，给出该单词翻译信息的页面`URL`为：`http://dict.youdao.com/w/xxxxx`

由于我要抓取词`xxxxx`中查询结果的所有双语例句，则需要在页面下部的显示最后一个例句后，点击`更多双语例句`，这会更新当前页面的`URL`为：`http://dict.youdao.com/example/blng/eng/xxxxx/#keyfrom=dict.main.moreblng`，所有双语例句均在其中，因此，我们只对这个`URL`进行爬取分析即可。

输入以下代码，并观察执行结果（查看所保存文件内容）。

```python
# coding: utf-8
# 示例代码 D-5
import requests
from bs4 import BeautifulSoup

def get_word_sents(word):
    '''
    根据给定的词汇返回该词汇(word)在有道词典查询页面中的所有双语例句
    参数：word，类型：str
    返回：含有双语例句的list
    '''
    url = 'http://dict.youdao.com/example/blng/eng/{}/#keyfrom=dict.main.moreblng'.format(word)
    r = requests.get(url)
    soup = BeautifulSoup(r.text, 'html.parser')
    
    sents = []
    for model in soup.find_all(attrs={"class":"ol"}):
        models = model.find_all('p')
        for model in models:
            try:
                model['class']
            except KeyError:
                sents.append(model.text.replace('\n','')+'\n)
    return sents
    
if __name__ == '__main__':
    word = '葡萄'
    sents = get_word_sents(word)
    
    filename = '1.txt'
    with open(filename, 'w', encoding = 'utf-8') as f:
        f.writelines(sents)
```
示例代码 D-5 中：
- url = `'http://dict.youdao.com/example/blng/eng/{}/#keyfrom=dict.main.moreblng'.format(word)`，构造查询URL
- `model['class']`，取`model`的`class`键的值，此处是为了过滤掉非双语例句的文本，形如`class="example-via"`等包含的文本将被过滤掉。

至此，给定任意词汇，已经可以抓取到其双语例句平行文本，并存到文件中。

对任意静态网页，均可以利用以上类似的方法，抓取感兴趣的内容。对更复杂的网页，需要对BeautifulSoap进一步进行了解，可参考其官方文档：`https://www.crummy.com/software/BeautifulSoup/bs4/doc/`，而有些网页很不规律特别是结构有问题的网页，也不太容易用beautifulSoap进行提取，这时，可以利用正则表达式，一般解析网页最经常的就是用BeautifulSoap和正则表达式联合解析。

4. 用正则表达式处理字符串

正则表达式，这个术语不太容易望文生义(没有去考证是如何被翻译为正则表达式的)，其实其英文为Regular Expression，直接翻译就是：有规律的表达式。这个表达式其实就是一个字符序列，反映某种字符规律，用(字符串模式匹配)来处理字符串。很多高级语言均支持利用正则表达式对字符串进行处理的操作。

4.1 示例

我们先看一个简单的正则表达式：E，所反映的字符规律是：以标签`<p>`及`</p>`包含的字符串(包含标签)。同时，为了便于观察是否反映这一规律，我们将以一个字符串作为输入，其中会有包含这种字符规律的部分，并利用正则表达式E，将符合这种规律的字符串进行提取。

```python
#coding: utf-8
#示例程序 D-6
import re

line ='asdfjd<p>adfasdQ  sdkf74j</p>okik'
print(re.search(r'<p>.+</p>', line)) 
```
示例程序D-6中：
- `import re`，引入re模块，即引入正则表达式模块。
- `re.search(r'<p>.+</p>', line)`，调用re模块的search()函数，其中有两个参数，第一个参数是一个字符串`r'<p>.+</p>'`，反映字符模式，也就是正则表达式；第二个参数是目标字符串，即变量line。整个功能是要从目标字符串line中搜索符合模式`r'<p>.+</p>'`的字符串。

从示例程序D-6中，我们发现程序会将以标签`<p>`及`</p>`包含的字符串(包含标签)全部提取出来，因此可以合理猜测上面的正则表达式中`<p>`及`</p>`表示的仍然是自己，而`.+`可能表示多个字符(获取是任意字符，不限定个数)。

示例程序D-6也是利用正则来进行字符串处理的一般范式：1. 观察字符串；2. 设计正则表达式；3. 进行匹配提取。

5. scrapy爬虫框架
