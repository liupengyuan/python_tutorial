{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By liupengyuan[at]pku.edu.cn\n",
    "### Project: https://github.com/liupengyuan/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 什么是爬虫\n",
    "简而言之，爬虫就是一段能够获取互联网信息（数据）的程序/工具。\n",
    "\n",
    "一般需要通过抓取网页来获取互联网的信息与数据。\n",
    "\n",
    "网页本身就是一个本文文件，只不过这个文本文件是由特定规则和符号标记的(HTML,超文本标记语言)，称为超文本文件，也可称为网页源代码。\n",
    "\n",
    "这段文本经过浏览器的解析（各类图片视频等在此过程中从网页外部加载），就成为我们日常浏览的网页展示给我们的样子了。\n",
    "\n",
    "因此，爬虫首先就是要获得网页这个文本文件，并进行解析。\n",
    "\n",
    "如果所需数据就直接在网页文本文件中，则可直接得到；如果所需数据在网页外部，则通过解析的结果得到数据所在地址，将该数据下载。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 最简爬虫**\n",
    "\n",
    "这里我们以优秀的python第三方库requests为例，该库是“唯一一个非转基因的Python HTTP库，人类可以安全享用”。\n",
    "\n",
    "其中HTTP(HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议，超文本文件传输时，必须遵守这个协议。\n",
    "\n",
    "非转基因是作者幽默的说法，表明该库是原汁原味符合python设计理念，易用易于理解。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入必要的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://github.com/liupengyuan/python_tutorial/blob/master/chapter1/0.md')\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- import requests，首先导入requests库\n",
    "- r = requests.get('https://github.com/liupengyuan/python_tutorial/blob/master/chapter1/0.md')，调用requests的get方法，向网址https://github.com/liupengyuan/python_tutorial/blob/master/chapter1/0.md 发送获取(get)请求，该方法返回一个应答(Response)对象r，其中包含该网页的所有信息。\n",
    "- print(r.text[:1000])，利用对象r的text属性字符串，打印网页内容，因为内容较多，暂取前1000个字符。\n",
    "- 打开https://github.com/liupengyuan/python_tutorial/blob/master/chapter1/0.md网页，在网页正文区域点击右键，选择查看源代码，会发现代码示例D-1确实已经获取/抓取这个网页的文本文件。\n",
    "- 至此，一个最简爬虫已经完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 静态定向爬虫基础**\n",
    "\n",
    "- 本小节中的爬虫是开始就指定了要抓取网页的地址（定向），要抓取的内容直接存在在网页源代码中（静态），因此称为静态定向爬虫。\n",
    "\n",
    "- 由于需要用到Chrome浏览器中的**检查**功能，因此需要读者下载安装Chrome浏览器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 抓取网易首页(www.163.com)上的全部新闻**\n",
    "\n",
    "- 我们打开网易首页，然后用鼠标右键点击页面，用chrome浏览器可选择检查，进入开发者模式。\n",
    "- 发者模式中，右侧为开发者工具面板，右上部的Elements标签页面中显示一片代码，是该网页html标记的文本（经过整理对齐的）。\n",
    "- 推荐教程：http://www.w3school.com.cn/html/index.asp ，快速浏览，可了解一些html基础知识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://www.163.com/')\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用鼠标右键在新闻标题上点击，并选择`检查`，可在Elements标签页面找到该新闻标题以及对应的链接在html代码中的位置。\n",
    "- 代码大致如下形式：\n",
    "```\n",
    "<li class=\"cm_fb\">\n",
    "<a href=\"http://news.163.com/xxxxxx.html\">yyyyyyyyy</a>\n",
    "::after\n",
    "</li>\n",
    "```\n",
    "- 其中xxxxx及yyyyyyy随新闻标题不同而不同。前者是新闻链接，后者新闻是标题，因此这两项是我们感兴趣并希望抓取的内容\n",
    "- 在Elements标签页面中继续查看各个新闻标题，可以发现各个新闻其实都在标签对`<li>`及`</li>`之间，且没有其间没有换行符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = r'<li>(.+)?</li>'\n",
    "contents = re.findall(p, r.text)\n",
    "print('\\n'.join(contents[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `p = r'<li>(.+)?</li>'` 构造了正则表达式，可提取`<li>...</li>`之间的内容(不包括之间的换行符)\n",
    "- `re.findall(p, r.text)` 将r.text中所有的符合上正则的提取出来放入列表\n",
    "- 最终打印前50行，可以发现后面有一些并非新闻标题与对应链接，并非所有在`<li>...</li>`之间的内容都是目标内容\n",
    "\n",
    "- 有关正则表达式，可参考教程：https://github.com/liupengyuan/python_tutorial/blob/master/chapter3  中的正则表达式快速教程。可以通过构造一些正则表达式来完成网页解析将目标内容提取。\n",
    "- 这里我们将介绍一个优秀的第三方网页解析包：Beautifulsoap。该包已经随Anaconda安装，名称为bs4。可以用这个包(必要时联合正则表达式)来进行网页解析。\n",
    "- 该包已经通过`from bs4 import BeautifulSoup`在开始导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "contents = soup.find_all('ul', attrs='cm_ul_round')\n",
    "print('type is:', type(contents))\n",
    "contents[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数BeautifulSoup(r.text, 'html.parser')，返回一个BeautifulSoup对象。第一个参数为要解析的网页文本，第二个参数是解析器的选择，暂选择python内置的'html.parser'作为解析器\n",
    "- soup.find_all('ul', attrs='cm_ul_round')，BeautifulSoup对象的方法，可返回指定标签之间的所有内容的ResultSet对象。其中第一个参数为标签，第二个参数为标签的属性。根据新闻标题，在Elements标签页面稍加分析可知：新闻标题及链接均在标签对`<ul class=\"cm_ul_round\">`及`</ul>`之间，且其他非新闻标题的内容，均不在这种规定了class的`<ul>`标签之间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in contents:\n",
    "    url_titles = line.find_all('a')\n",
    "    for url_title in url_titles:\n",
    "        url = url_title.get('href')\n",
    "        title = url_title.string\n",
    "        print(type(url_title), url, title)\n",
    "    if input()=='b':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可对ResultSet进行遍历，利用`line.find_all('a')`取得其中在`<a>`及`</a>`标签对(该标签对表示超链接)中间的内容，link_texts的内容仍然是一个ResultSet对象，含有所有的link及text\n",
    "- 对link_texts进行遍历，每一个对象均为`Tag`对象，利用`Tag`对象的`link_text.get('href')`方法，该方法可以取得标签的属性值，本例中参数为属性`href`，其值为超链接的URL\n",
    "- 利用标签的`string`属性，取得该超链接的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_title_dict = {}\n",
    "for line in contents:\n",
    "    url_titles = line.find_all('a')\n",
    "    for url_title in url_titles:\n",
    "        url = url_title.get('href')\n",
    "        title = url_title.string\n",
    "        if url and url.endswith(r'.html'):\n",
    "            try:\n",
    "                url_title_dict[url] = title\n",
    "            except KeyError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这里暂只需要得到首页的标题及对应的新闻页面链接，因此只抽取链接结尾为`.html`的条目\n",
    "- 所有结果存入到词典url_title_dict中\n",
    "\n",
    "我们还要获取各链接下的新闻文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_text_dict = {}\n",
    "for url, title in url_title_dict.items():\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    html = soup.find('div', attrs = 'post_text')\n",
    "    if html:\n",
    "        passages = html.find_all('p')\n",
    "        text = ''\n",
    "        for passage in passages:\n",
    "            if passage.string:\n",
    "                text += passage.string\n",
    "\n",
    "        title_text_dict[url] = title, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过点击新闻页面，可以发现，所有正文在`<div class = 'post_text>...</div>`中\n",
    "- 通过soup对象的find()方法，可以得到上述标签中的内容，存放在html变量中\n",
    "- 内容是由`<p>...</p>`标签对之间(p即passage)，因此需要利用find_all()方法提取之中所有的段落，放入passages中\n",
    "- 遍历passages，其中每一个passage均是Tag对象，利用Tag对象的string方法，得到该标签对应的文本\n",
    "\n",
    "至此，形成了一个词典，key为新闻的url，value为对应的标题与正文，可将上述程序整合，并将抓取内容保存到文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_163_home_news(filename):\n",
    "    try:\n",
    "        r = requests.get(r'http://www.163.com')\n",
    "    except:\n",
    "        print('Can not get the page.\\n')\n",
    "        return False\n",
    "    \n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    contents = soup.find_all('ul', attrs='cm_ul_round')\n",
    "    \n",
    "    error_url = []\n",
    "    f_err = open('error_urls', 'w', encoding = 'utf-8')\n",
    "    fh = open(filename, 'w', encoding = 'utf-8')\n",
    "    for line in contents:\n",
    "        url_titles = line.find_all('a')\n",
    "        for url_title in url_titles:\n",
    "            url = url_title.get('href')\n",
    "            title = url_title.string\n",
    "            if url and url.endswith(r'.html'):\n",
    "                try:\n",
    "                    r = requests.get(url)\n",
    "                except:\n",
    "                    print('Error in getting:', url)\n",
    "                    error_url.append(url)\n",
    "                    continue\n",
    "                soup = BeautifulSoup(r.text, 'html.parser')\n",
    "                html = soup.find('div', attrs = 'post_text')\n",
    "                if html:\n",
    "                    passages = html.find_all('p')\n",
    "                    text = ''\n",
    "                    for passage in passages:\n",
    "                        if passage.string:\n",
    "                            text += passage.string\n",
    "                    fh.write('{}\\n{}\\n{}\\n'.format(url, title, text))\n",
    "    \n",
    "    f_err.write('\\n'.join(error_url))\n",
    "    fh.close()\n",
    "    f_err.close()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = r'news_163_home.text'\n",
    "get_163_home_news(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 抓取有道词典查询词的双语例句**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析：\n",
    "\n",
    "- 通过在有道词典(dict.youdao.com)进行单词查询，发现对任意单词xxxxx，给出该单词翻译信息的页面URL为：http://dict.youdao.com/w/xxxxx\n",
    "\n",
    "- 由于我要抓取词xxxxx中查询结果的所有双语例句，则需要在页面下部的显示最后一个例句后，点击更多双语例句，这会更新当前页面的URL为：http://dict.youdao.com/example/blng/eng/xxxxx/#keyfrom=dict.main.moreblng 。所有双语例句均在其中，因此，我们只对这个URL进行爬取分析即可\n",
    "- 输入一个中文词汇进行查询，选中第一个双语例句，右键点击并选择检查，进入chrome开发者模式，发现所有例句均在`<ul class='ol'>...</ul>`标签对之间。\n",
    "- 每个例句均在`<p>...</p>`标签对之间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_word_sents(word):\n",
    "    url = r'http://dict.youdao.com/example/blng/eng/{}/#keyfrom=dict.main.moreblng'.format(word)\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except:\n",
    "        print('Can not get the page.\\n')\n",
    "        return False\n",
    "    \n",
    "    word_sents = []\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    contents = soup.find('ul', attrs = 'ol')\n",
    "    sents = contents.find_all('p')\n",
    "    for sent in sents:\n",
    "        word_sents.append(sent.text.replace('\\n','')+'\\n')\n",
    "    \n",
    "    return word_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 整个过程与网易新闻抓取的方法类似\n",
    "- `word_sents.append(sent.text.replace('\\n','')+'\\n')`，是为了写入文件时候较为整齐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "test_word = '爬虫'\n",
    "sents = get_word_sents(test_word)\n",
    "with open(test_word+'.txt', 'w', encoding = 'utf-8') as f:\n",
    "    f.writelines(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 动态页面的抓取(以后)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Scrapy爬虫框架(以后)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
